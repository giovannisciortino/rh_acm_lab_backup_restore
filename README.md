# Introduction
This repository contains ansible playbooks to setup an OpenShift lab in order to execute the backup and restore procedure of Red Hat Advanced Cluster Management based on the official Red Hat documentation [1].

The prerequisites to configure the lab are:

 1. An ARO OpenShift 4.12 cluster on Azure
 2. A ROSA Openshift 4.12 cluster on AWS
 3. Another Openshift 4.12 cluster that will be configured as managed cluster in ACM
 4. The credentials (access key and secret key) of an AWS account used to configure an S3 bucket where the ACM bucket will be stored

The ansible playbooks configures the OpenShift clusters as described below:

 - A Red Hat ACM 2.8 is configured on the ARO Openshift Cluster. This ACM is named "Primary ACM HUB" in the remaining part of the document. This is the active hub where the backup will be initially configured and scheduled. In order to simualte an ACM workload, the playbooks imports a managed cluster on this ACM and an example ACM policy will be deployed on this managed cluster by ACM. 
 - The backup of the Primary ACM HUB is stored in a S3 Bucket on AWS
 - A Red Hat ACM 2.8 is configured on the ROSA OpenShift Cluster.  This ACM is named "Seconday ACM HUB" in the remaining part of the document. The playbooks apply the configuration needed to restore the backup on this ACM instance

This repository contains three playbooks:

 - **install_lab.yml** : This playbook executes the following tasks:
   - Install primary and secondary ACM HUB
   - Import the managed cluster on the primary HUB
   - Deploy an example policy ( contained in the file acm_deploy_simple_policy/templates/09_Policy_hello-world.yml ) that create a namespace named "hello-world" on the managed cluster
   - Create a S3 bucket on AWS to store ACM backup
   - Configure and schedule the backup on primary ACM HUB
   - Configure the restore configuration on secondary ACM HUB without execute the restore
   
-  **start_restore_to_secondary_hub.yml**: This playbook executes the following tasks:
   - Stop the pods of the primary ACM HUB
   - Start the restore process on secondary ACM HUB
   - Schedule the backup of secondary ACM HUB every hour
  
 - **start_rollback_to_primary_hub.yml**: This playbook executes the following tasks:
   - Stop the pods of the secondary ACM HUB
   - Start the pods of the primary ACM HUB
   - Start the restore process on primary ACM HUB
   - The schedule of backup of primary ACM HUB configured by playbook is restarted when the pod of primary ACM Hub are started

# Usage

 1. Edit the file **00_lab_configuration_variables.yaml** and insert the following variables:
 
| Variable name  | Description |
|--|--|
| primary_hub_ocp_host | The OpenShift API URL of the cluster where the primary ACM HUB will be installed|
| primary_hub_ocp_api_key | An API token of the OpenShift cluster where the primary ACM HUB will be installed. The API token must be generated by a user with cluster-admin privileges |
| primary_hub_ocp_validate_certs | A boolean value. It can be set to the value "True" if the TLS certificates of the OpenShift cluster(where the primary ACM HUB is installed) must be validated and they aren't self signed. Otherwise this value can be set to the value "False" to skip the validation of API TLS certificates |
| secondary_hub_ocp_host | The OpenShift API URL of the cluster where the secondary ACM HUB will be installed|
| secondary_hub_ocp_api_key | An API token of the OpenShift cluster where the secondary ACM HUB will be installed. The API token must be generated by a user with cluster-admin privileges |
| secondary_hub_ocp_validate_certs | A boolean value. It can be set to the value "True" if the TLS certificates of the OpenShift cluster(where the secondary ACM HUB is installed) must be validated and they aren't self signed. Otherwise this value can be set to the value "False" to skip the validation of API TLS certificates |
| managed_cluster_ocp_host | The Openshift API URL of the cluster that will be imported as managed cluster in ACM |
| managed_cluster_ocp_api_key | An API token of the OpenShift cluster that will be imported as managed cluster in ACM. The API token must be generated by a user with cluster-admin privileges |
| managed_cluster_ocp_validate_certs | A boolean value. It can be set to the value "True" if the TLS certificates of the OpenShift cluster imported as managed cluster must be validated and they aren't self signed. Otherwise this value can be set to the value "False" to skip the validation of API TLS certificates |
| managed_cluster_name | The name on ACM of the Openshift cluster imported as managed cluster |
| aws_access_key | The access key of an AWS account where the S3 bucket used to store ACM backup will be configured |
| aws_secret_key | The secret key of an AWS account where the S3 bucket used to store ACM backup will be configured | 

2. Execute the script **01_create_install_lab_python_venv.sh** . This script creates a python virtual environment with the python libraries needed by the ansible playbooks contained in this repository

3. Execute the script **02_install_lab.sh** . This script loads the python virtual environment and execute the playbook install_lab.yml

4. Wait that the first backup of the primary ACM HUB is completed. The backup is scheduled at the minute 0 of each hour. This schedule can be modified editing the variable velero_schedule of the file /home/gsciorti/Documents/acm_lab/rh_acm_lab_backup_restore/roles/acm_backup_configure/vars/main.yaml

5. Execute the script **03_start_restore_to_secondary_hub.sh**. This script loads the python virtual environment and execute the playbook start_restore_to_secondary_hub.yml

6. Wait that the first backup of the secondary ACM HUB is completed. The backup is scheduled at the minute 0 of each hour. This schedule can be modified editing the variable velero_schedule of the file /home/gsciorti/Documents/acm_lab/rh_acm_lab_backup_restore/roles/acm_backup_configure/vars/main.yaml

8. Execute the script **04_start_rollback_to_primary_hub.sh**. This script loads the python virtual environment and execute the playbook start_rollback_to_primary_hub.yml

# Ansible Role description

The playbooks contained in this repository import ansible roles executing the requested tasks.
The ansible roles are contained in the directory roles of this repository.
The following table contains a description of the ansible roles:
| Ansible role name | Description |
|--|--|
| acm_install | This role installs and configures Red Hat ACM on a OpenShift cluster. The same role using different variables is used to configure the primary and the secondary ACM HUB|
| acm_import_managedcluster | This role imports a managed cluster on the primary ACM HUB. This ansible role is based on an existing ansible role available on GitHub [2]. The version included in this repository has been modified to import the managed cluster using an API token to authenticate the playbook on ACM HUB and on the managed cluster |
| acm_deploy_simple_policy | This ansible role deploys a simple ACM Policy on the OpenShift cluster imported as managed cluster in the primary ACM HUB. The policy is assigned to the managed cluster using a Placement resource and a PlacementBinding resource. The policy creates a namespace named "hello-world" on the managed cluster |
| acm_backup_configure | This ansible role configures the backup on the primary RH ACM HUB and schedule the execution of the backup every hour.<br><br> It executes the following tasks:<br><br> 1) Create a S3 bucket <br><br>2) Create a secret containing S3 bucket credential <br><br>3) Create the ACM DataProtectionApplication resource<br><br>4) Create the ACM BackupSchedule resource<br><br> |
| acm_restore_configure | This ansible role configures the backup on the primary RH ACM HUB. It executes the following tasks:<br> 1) Create a S3 bucket <br><br>2) Create a secret containing S3 bucket credential <br><br>3) Create the ACM DataProtectionApplication resource<br><br>4) Create the ACM BackupSchedule resource<br><br> |
| acm_start_restore_to_secondary_hub | This ansible role executes the restore process on the secondary ACM HUB.<br> It executes the following tasks:<br><br>1) Stop and disable the scheduling of the pods running on the namespaces "open-cluster-management","open-cluster-management-hub" and "open-cluster-management-backup" of the primary ACM HUB. This task is executed to simulate the shutdown of the OpenShift cluster where the primary ACM Hub is installed. This task is performed adding a node-selector annotation with a not-existing label to these namespaces and deleting the existing pods running on these namespaces.<br><br> 2) Create the ACM Restore resource to start the restore process on the secondary ACM HUB<br><br>Note: The restore process uses the "automatic import" feature[4] to import automatically the managed cluster in the secondary ACM HUB. This feature is currently in technology preview state.<br><br>3) Create ACM BackupSchedule resource on the secondary ACM HUB    |
| acm_start_rollback_to_primary_hub | This ansible role executes the rollback process to the primary ACM HUB.<br> It executes the following tasks:<br><br> 1) Stop and disable the scheduling of the pods running on the namespaces "open-cluster-management","open-cluster-management-hub" and "open-cluster-management-backup" of the secondary ACM HUB. This task is executed to simulate the shutdown of the OpenShift cluster where the secondary ACM Hub is installed. This task is performed adding a node-selector annotation with a not-existing label to these namespaces and deleting the existing pods running on these namespaces.<br><br> 2) Enable the scheduling and start the pods of the pods on the namespaces "open-cluster-management","open-cluster-management-hub" and "open-cluster-management-backup" of the primary ACM HUB. This task is performed removing the node-selector annotation to these namespaces and deleting the existing pods running on these namespaces. <br><br> 3) Create the ACM Restore resource to start the restore process on the secondary ACM HUB<br><br>Note: The restore process uses the "automatic import" feature[4] to import automatically the managed cluster in the secondary ACM HUB. This feature is currently in tecnology preview state.<br><br> |

# Point of attention

1. The "automatic import" feature[4] using during the restore process in currently in technology preview state.
2. The official documentation of ACM restore process describe in the paragraph [4] to shutdown the cluster where the backup is executed before to start the restore process. The shutdow of ARO/ROSA OpenShift cluster isn't yet a supported task [5]. In order to simulate the shutdown of the ACM HUB, the playbooks contained in this repository add a node-selector annotation with a not-existing label to the namespaces "open-cluster-management","open-cluster-management-hub" and "open-cluster-management-backup" and restart the pod running on these namespaces. This is not an officially supported procedure.


# Reference
[1] https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html-single/business_continuity/index#backup-intro

[2] https://github.com/sdminonne/backup-n-restore-imported-clusters/tree/main/roles/import-managedcluster

[3] https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html-single/business_continuity/index#primary-cluster-shut-down

[4] https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html-single/business_continuity/index#auto-connect-clusters-msa

[5] https://access.redhat.com/solutions/6193451